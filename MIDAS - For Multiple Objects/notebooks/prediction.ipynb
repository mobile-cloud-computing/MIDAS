{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import ntpath\n",
    "import os\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Extract Frames from Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_to_frames(input_loc, output_loc):\n",
    "    \"\"\"Function to extract frames from input video file\n",
    "    and save them as separate frames in an output directory.\n",
    "    Args:\n",
    "        input_loc: Input video file.\n",
    "        output_loc: Output directory to save the frames.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.mkdir(output_loc)\n",
    "    except OSError:\n",
    "        pass\n",
    "    # Log the time\n",
    "    time_start = time.time()\n",
    "    # Start capturing the feed\n",
    "    cap = cv2.VideoCapture(input_loc)\n",
    "    # Find the number of frames\n",
    "    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1\n",
    "    print (\"Number of frames: \", video_length)\n",
    "    count = 0\n",
    "    print (\"Converting video..\\n\")\n",
    "    # Start converting the video\n",
    "    while cap.isOpened():\n",
    "        # Extract the frame\n",
    "        ret, frame = cap.read()\n",
    "        #print(frame.shape)\n",
    "        #print(frame[0])\n",
    "        #print(np.amax(frame))\n",
    "        #break\n",
    "        # Write the results back to output location.\n",
    "        cv2.imwrite(output_loc + \"/%#05d.jpg\" % (count+1), frame)\n",
    "        count = count + 1\n",
    "        # If there are no more frames left\n",
    "        if (count > (video_length-1)):\n",
    "            # Log the time again\n",
    "            time_end = time.time()\n",
    "            # Release the feed\n",
    "            cap.release()\n",
    "            # Print stats\n",
    "            print (\"Done extracting frames.\\n%d frames extracted\" % count)\n",
    "            print (\"It took %d seconds for conversion.\" % (time_end-time_start))\n",
    "            break\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frameconversion():\n",
    "    print (\"\\n Converting frames . . .\\n\")\n",
    "    filelist=glob.glob('C:/Users/dar/Desktop/Midas - Multiple Objects/frames/*.jpg')\n",
    "    for i in filelist:\n",
    "        image = cv2.imread(i)\n",
    "        #print(type(image))\n",
    "        #print(image[0])\n",
    "\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        blurred = cv2.GaussianBlur(gray, (11, 11), cv2.BORDER_DEFAULT)\n",
    "        ret,th1 = cv2.threshold(blurred,170,255,cv2.THRESH_BINARY)\n",
    "\n",
    "\n",
    "        # gaussian_thresh= cv2.adaptiveThreshold(blurred,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,201,0)\n",
    "\n",
    "        #cv2.imshow(\"binary\",th1)\n",
    "\n",
    "        #cv2.waitKey(0)\n",
    "        #cv2.destroyAllWindows()\n",
    "\n",
    "        # original = image.copy()\n",
    "        # gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        # blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "        # #canny = cv2.Canny(blurred, 120, 255, 1)\n",
    "        # #kernel = np.ones((5,5),np.uint8)\n",
    "        # #dilate = cv2.dilate(canny, kernel, iterations=1)\n",
    "        filename=ntpath.basename(i)\n",
    "        pre=filename.split(\".\")[0]\n",
    "        fi='C:/Users/dar/Desktop/Midas - Multiple Objects/step2/'+pre+\".jpg\"\n",
    "        cv2.imwrite(fi,th1)\n",
    "    print (\"Applied Gaussian Blur and Threshold . . .\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove the FLIR Logo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeflir():\n",
    "    filelist=glob.glob('C:/Users/dar/Desktop/Midas - Multiple Objects/step2/*.jpg')\n",
    "    for i in filelist:\n",
    "        image=cv2.imread(i)\n",
    "        rectangle = np.array([[3,475],[160, 475], [165, 405], [7, 405]])\n",
    "        color = [0, 0, 0] #black\n",
    "        cv2.fillConvexPoly(image, rectangle, color)\n",
    "        filename=ntpath.basename(i)\n",
    "        pre=filename.split(\".\")[0]\n",
    "        fi='C:/Users/dar/Desktop/Midas - Multiple Objects/step2/'+pre+\".jpg\"\n",
    "        cv2.imwrite(fi,image)\n",
    "    print (\"Removed FLIR watermark . . .\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist=glob.glob('C:/Users/dar/Desktop/Midas - Multiple Objects/step2/*.jpg')\n",
    "image_number = 0\n",
    "for i in filelist:\n",
    "    image = cv2.imread(i)\n",
    "    original = image.copy()\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "    canny = cv2.Canny(blurred, 120, 255, 1)\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    dilate = cv2.dilate(canny, kernel, iterations=1)\n",
    "\n",
    "    # Find contours\n",
    "    cnts = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "    obj=1\n",
    "    for c in cnts:\n",
    "        if(obj==1):\n",
    "            x1,y1,w1,h1 = cv2.boundingRect(c)\n",
    "            obj+=1\n",
    "        else:\n",
    "            x2,y2,w2,h2 = cv2.boundingRect(c)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate the Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectseparation():\n",
    "    filelist=glob.glob('C:/Users/dar/Desktop/Midas - Multiple Objects/step2/*.jpg')\n",
    "    image_number = 0\n",
    "    for i in filelist:\n",
    "        image = cv2.imread(i)\n",
    "        original = image.copy()\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "        canny = cv2.Canny(blurred, 120, 255, 1)\n",
    "        kernel = np.ones((5,5),np.uint8)\n",
    "        dilate = cv2.dilate(canny, kernel, iterations=1)\n",
    "\n",
    "\n",
    "        for j in range(2):\n",
    "            if(j==0):\n",
    "                x,y,w,h=x1,y1,w1,h1\n",
    "                path=\"C:/Users/dar/Desktop/Midas - Multiple Objects/OBJ1/\"\n",
    "            else:\n",
    "                x,y,w,h=x2,y2,w2,h2\n",
    "                path=\"C:/Users/dar/Desktop/Midas - Multiple Objects/OBJ2/\"\n",
    "            #x,y,w,h = cv2.boundingRect(c)\n",
    "            #print(x,y,w,h)\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), (36,255,12), 2)\n",
    "            #cv2.rectangle(image, (x, y), (x + w, y + h), (36,255,12), 2)\n",
    "            ROI = original[y-30:y+h+30, x-30:x+w+30]\n",
    "            cv2.imwrite(path+\"{}.jpg\".format(image_number), ROI)\n",
    "            image_number += 1\n",
    "            #area = cv2.contourArea(c)\n",
    "            #print(area)\n",
    "    #     cv2.imshow('canny', canny)\n",
    "    #     cv2.imshow('image', image)\n",
    "    #     cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Get Information about White Parts \n",
    "def datapreprocessing(objectlocation):\n",
    "    filelist=glob.glob(objectlocation)\n",
    "    result_array = np.array([])\n",
    "\n",
    "    counter = 0\n",
    "    for i in filelist:\n",
    "        if counter == 20:\n",
    "            counter = 0\n",
    "            image = cv2.imread(i)\n",
    "            occurrences = np.count_nonzero(image == 255)\n",
    "            result_array = np.append(result_array, occurrences)\n",
    "            #print(occurrences)\n",
    "        counter += 1\n",
    "    #print(type(result_array[0]))\n",
    "    \n",
    "    ## Normalise The DATA and save the Normalised Data to NPZ File\n",
    "\n",
    "    startvalue=result_array[0]\n",
    "\n",
    "    data=np.array([])\n",
    "\n",
    "    counter = 0\n",
    "    sample = []\n",
    "    samples = []\n",
    "    #extract normalised data\n",
    "    for i in result_array:\n",
    "        if counter == 10:\n",
    "            samples.append(sample)\n",
    "            sample = []\n",
    "            counter = 0\n",
    "        devvalue=float(startvalue-i)/float(startvalue)\n",
    "    #     print(devvalue)\n",
    "        normvalue=1-devvalue\n",
    "        #print(normvalue)\n",
    "        sample.append(normvalue)\n",
    "        counter += 1\n",
    "\n",
    "    samples_np = np.array(samples)\n",
    "    samples_np.shape \n",
    "    print(\"Created .npz file\")\n",
    "    #     data=np.append(data,normvalue)\n",
    "    return samples\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixedholdprediction(test):\n",
    "    data = test['samples']\n",
    "    X=np.array(data)\n",
    "    \n",
    "    # Load the Saved Model back from file\n",
    "    with open(\"C:/Users/dar/Desktop/Midas - Multiple Objects/saved models/Fixedhold_Modell.pkl\", 'rb') as file:  \n",
    "        Model = pickle.load(file)\n",
    "    result=Model.predict(X)\n",
    "    uniqueclasses,count_class=np.unique(result,return_counts=True)\n",
    "    predictedindex=np.argmax(count_class)\n",
    "    classresult=uniqueclasses[predictedindex]\n",
    "    return classresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naturalholdprediction(test):\n",
    "    data = test['samples']\n",
    "    X=np.array(data)\n",
    "    \n",
    "    # Load the Saved Model back from file\n",
    "    with open(\"C:/Users/dar/Desktop/Midas - Multiple Objects/saved models/Naturalhold_Modell.pkl\", 'rb') as file:  \n",
    "        Model = pickle.load(file)\n",
    "    result=Model.predict(X)\n",
    "    uniqueclasses,count_class=np.unique(result,return_counts=True)\n",
    "    predictedindex=np.argmax(count_class)\n",
    "    classresult=uniqueclasses[predictedindex]\n",
    "    return classresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinedprediction(test):\n",
    "    data = test['samples']\n",
    "    X=np.array(data)\n",
    "    \n",
    "    # Load the Saved Model back from file\n",
    "    with open(\"C:/Users/dar/Desktop/Midas - Multiple Objects/saved models/Combined_Model.pkl\", 'rb') as file:  \n",
    "        Model = pickle.load(file)\n",
    "    result=Model.predict(X)\n",
    "    uniqueclasses,count_class=np.unique(result,return_counts=True)\n",
    "    predictedindex=np.argmax(count_class)\n",
    "    classresult=uniqueclasses[predictedindex]\n",
    "    return classresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frames:  876\n",
      "Converting video..\n",
      "\n",
      "Done extracting frames.\n",
      "876 frames extracted\n",
      "It took 7 seconds for conversion.\n",
      "\n",
      " Converting frames . . .\n",
      "\n",
      "Applied Gaussian Blur and Threshold . . .\n",
      "\n",
      "Removed FLIR watermark . . .\n",
      "\n",
      "Created .npz file\n",
      "Created .npz file\n",
      "\n",
      "For Object 1: \n",
      "\n",
      "Fixed Hold Model Result: The material belongs to Class :  1\n",
      "Natural Hold Model Result: The material belongs to Class :  1\n",
      "Combined Model Result: The material belongs to Class :  1\n",
      "\n",
      "For Object 2: \n",
      "\n",
      "Fixed Hold Model Result: The material belongs to Class :  1\n",
      "Natural Hold Model Result: The material belongs to Class :  1\n",
      "Combined Model Result: The material belongs to Class :  1\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    input_loc = 'C:/Users/dar/Desktop/Midas - Multiple Objects/Video1.mp4'\n",
    "    output_loc = 'C:/Users/dar/Desktop/Midas - Multiple Objects/frames'\n",
    "    video_to_frames(input_loc, output_loc)\n",
    "    frameconversion()\n",
    "    removeflir()\n",
    "    objectseparation()\n",
    "    samplesobj1=datapreprocessing('C:/Users/dar/Desktop/Midas - Multiple Objects/OBJ1/*.jpg')\n",
    "    np.savez_compressed('npzfileobj1', samples=samplesobj1)   #name of .npz file\n",
    "    samplesobj2=datapreprocessing('C:/Users/dar/Desktop/Midas - Multiple Objects/OBJ2/*.jpg')\n",
    "    np.savez_compressed('npzfileobj2', samples=samplesobj2)   #name of .npz file\n",
    "    \n",
    "    #Load the File for Object 1 \n",
    "    test = np.load('npzfileobj1.npz')\n",
    "    print(\"\\nFor Object 1: \\n\")\n",
    "    result1=fixedholdprediction(test)\n",
    "    result2=naturalholdprediction(test)\n",
    "    result3=combinedprediction(test)\n",
    "    print(\"Fixed Hold Model Result: The material belongs to Class : \", result1)\n",
    "    print(\"Natural Hold Model Result: The material belongs to Class : \", result2)\n",
    "    print(\"Combined Model Result: The material belongs to Class : \", result3)\n",
    "    \n",
    "    #Load the File for Object 2\n",
    "    test1 = np.load('npzfileobj2.npz')\n",
    "    print(\"\\nFor Object 2: \\n\")\n",
    "    result11=fixedholdprediction(test1)\n",
    "    result22=naturalholdprediction(test1)\n",
    "    result33=combinedprediction(test1)\n",
    "    print(\"Fixed Hold Model Result: The material belongs to Class : \", result11)\n",
    "    print(\"Natural Hold Model Result: The material belongs to Class : \", result22)\n",
    "    print(\"Combined Model Result: The material belongs to Class : \", result33)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
